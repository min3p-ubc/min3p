!*****Revision Informations Automatically Generated by VisualSVN*****!
!---------------------------------------------------------------------
!> $ID:$
!> $Revision: 850 $
!> $Author: dsu $
!> $Date: 2023-01-27 08:58:23 -0800 (Fri, 27 Jan 2023) $
!> $URL: https://min3psvn.ubc.ca/svn/min3p_thcm/branches/dsu_new_add_2024Jan/src/min3p/fsflow.F90 $
!---------------------------------------------------------------------
!********************************************************************!

!c ----------------------------------------------------------------------
!c subroutine fsflow
!c -----------------
!c
!c driver subroutine for fully saturated flow 
!c
!c written by:      Uli Mayer - May 6, 96
!c
!c last modified:   Uli Mayer - November 28, 96
!c
!c                  Danyang Su - Sept. 10, 2018
!c                  Unstructured grid and HPC capabilities
!c
!c definition of variables:
!c
!c I --> on input   * arbitrary  - initialized  + entries expected
!c O --> on output  * arbitrary  - unaltered    + altered
!c                                                                    I O
!c passed:   -
!c
!c common:   
!c gen.f:    real*8:
!c           -------
!c           avs(njavs)         = jacobian matrix                     * +
!c           afvs(njafvs)       = incomplete factorization            * +
!c           bvs(nn)            = rhs vector                          * +
!c           deltol_vs          = solver update tolerance             + -
!c           res_vs(nn)          = residual                            * *
!c           restol_vs          = solver residual tolerance           + -
!c           rmupdate           = maximum solution update (solver)    * +
!c           rnorm              = residual 2-norm                     * +
!c           rwork(8*nn)        = real*8 work array                   * *
!c           uvs(nn)            = update towards solution-vector      * +
!c           uvsnew(nn)         = solution vector (new time level)    + +
!c 
!c           integer*4:
!c           ----------
!c           igen               = unit number, generic output file    + -
!c           ilog               = unit number, logbook                + -
!c           idbg               = unit number, debugging information  + -
!c           iavs(nn+1)         = row pointer array for avs           + -
!c           iafvs(nn+1)        = row pointer array for afvs          + -
!c           iafdvs(nn)         = diagonal pointer array for afvs     + -
!c           idetail_vs         = solver information level            + -
!c           ittot_vs           = total number of iterations          + +
!c                                (variably saturated flow)
!c           iwork(2*nn+njafvs) = integer work array                  * *
!c           javs(njavs)        = connectivity list                   + -
!c           jafvs(njafvs)      = column pointer array for afvs       + -
!c           lordervs(nn)       = array containing ordering           + -
!c           invordvs(nn)       = array containing inverse ordering   + -
!c           nn                 = total number of control volumes     + -
!c           njavs              = number of global connections        + -
!c           njafvs             = number of factored connections      + -
!c           idbg               = unit number, debugging file         + -
!c           msolvit_vs         = max. number of solver iterations    + -
!c           itsolv             = actual number of solver iterations  * +
!c           itsolvtot_vs       = total number of solver              + +
!c                                iterations
!c                                (variably saturated flow)
!c           
!c           logical:
!c           --------
!c           transient_flow     = .true.  -> .not.steady_flow,        + -
!c                                        -> transient flow
!c  
!c
!c local:    integer*4:
!c           ----------
!c           ierr               = 0 -> memory allocation successful
!c           ilist              = pointer (integer work array)
!c           ivol               = counter (control volumes)
!c
!c           logical:
!c           --------
!c           over_flow          = .true.  -> ||r||_2 norm -> inf
!c
!c external: checkerr = check for error during memory allocation
!c           zero_r8   = clear real*8 array
!c           jacfs     = construct jacobian matrix for fully 
!c                       saturated flow problem
!c           jacbvs    = incorporate boundary terms in Jacobian 
!c                       matrix for flow problem
!c           incompletefactorization = incomplete lu-decomposition
!c                                     of jacobian matrix
!c           ws209     = iterative solution of matrix equation
!c ----------------------------------------------------------------------
 
      subroutine fsflow

#ifdef PETSC
#include <petscversion.h>
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 8)
#include <petsc/finclude/petscsys.h>
      use petscsys
#endif
#endif


      use parm
      use gen
      use dgml, only : dgm, maxwell 
      use solver_results, only : solver_results_check_output
      
#ifdef OPENMP
      use omp_lib 
#endif

#ifdef PARDISO
      use solver_pardiso, only : pardiso_symbolicfactorization,      &
                                 pardiso_numfactorization,           &
                                 pardiso_substitution, ptvs, iparm_vs
#endif

      
#ifdef PETSC
      use solver_dd, only : solver_dd_snes_solve_flow_heat
      use petsc_mpi_common, only : petsc_mpi_finalize
#endif

#ifdef LIS
      use solver_lis, only : solver_lis_solve_flow
#endif

      use file_unit, only : lun_get, lun_free
      use matrix_utility, only : export_arrays1d, export_mmformat,     &
                                 export_mmformat_gbl
      
      implicit none  
      
#ifdef PETSC
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 6 && PETSC_VERSION_MINOR < 8)
#include <petsc/finclude/petscsys.h>
#elif (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR < 6)
#include <finclude/petscsys.h>
#endif
#endif

#ifdef PETSC
      DOUBLE PRECISION :: mpireduce_in(2), mpireduce_out(2)
      integer*4 :: mpireduce_irank
      PetscErrorCode :: ierrcode
#endif

      real*8 :: uvsmax
      real*8, external :: cputime
      integer :: i, ierr, ilist, ivol, iter_sia, ifile, iskip, nskip,  &
                 idummy, n_unknown_vs, iter_div, ibvs, maxvol

      character*256 :: strdummy, strfile

      logical over_flow, b_redo_symbfac, b_freezing_pond
      
      real*8, parameter :: r0 = 0.0d0
      integer, parameter :: i0 = 0

#ifdef DEBUG
      integer :: tid, ntid
#endif

      external checkerr, zero_r8, jacfs, jacbvs, ws209,                &
              incompletefactorization,  updatevs, infcvs_cp, tstepfs

      iter_sia = 0

      iter_vs = 0

      if (b_use_zero_flow_vel) then
        not_converged = .false.
      else
        not_converged = .true.
      end if

      do while (not_converged)          !newton iteration loop
 
      prt_flow_tot = cputime()
      
!c  total number of iterations  =  1 / time step

      iter_vs = iter_vs+1             !iteration counter (current)
      ittot_vs = ittot_vs+1

      if (idetail_vs.eq.2 .and. rank == 0 .and. b_enable_output) then
        write(ilog,'(/a,i3,a)') 'Newton iteration ',iter_vs,':'
        write(ilog,'(a)') '---------------------'
      end if

!c  allocate memory for solver
      
      if (.not. allocated(avs)) then
          allocate (avs(njavs), stat = ierr)
          avs = 0.0
          call checkerr(ierr,'avs',ilog)
          call memory_monitor(sizeof(avs),'avs',.true.)
      end if
      
      if (i_solver_type_flow == 0) then
          if (.not. allocated(afvs)) then
              allocate (afvs(njafvs), stat = ierr)
              afvs=0.0d0 
              call checkerr(ierr,'afvs',ilog)
              call memory_monitor(sizeof(afvs),'afvs',.true.)
          end if
      end if

!c  clear arrays
  
      call zero_r8(avs,njavs,1,1)
  
      call zero_r8(bvs,nngl,1,1)
  
      call zero_r8(uvs,nngl,1,1)

!c  assemble matrix and rhs-vector
      prt_flow_jac = cputime()
  
!c  compute influence coefficients in terms of conductivities(jan,2003)
      if (gas_advection .or. dgm .or. maxwell) then
        call infcvs_cp
      end if  
          
!c   assemble matrix and rhs-vector
      call jacfs
      !Export sparse matrix dataset and right hand side. For test only, dsu.
      if((b_output_matrix.or.itimestep_output_matrix == mtime .or.     &
          (itimestep_output_matrix > 0 .and. mtime == 0)).and.         &
          b_enable_output) then
          if(itype_matrix_format == 0) then
              call export_arrays1d(nngl, njavs, iavs, javs, avs, bvs,  &
              uvs, .true., .true., .false., "fsflow_vs_exb", ittot_vs)
          else if(itype_matrix_format == 1) then
              call export_mmformat(nngl, njavs, iavs, javs, avs, bvs,  &
              uvs, .true., .true., .false., "fsflow_vs_exb", ittot_vs)
#ifdef PETSC
              call export_mmformat_gbl(nn,nngl,njavs,iavs,javs,avs,    &
                   bvs,uvs,.true.,.true.,.false.,"fsflow_vs_exb",      &
                   nngl,nngbl,.false.,ittot_vs)
#endif
          end if
      end if
    
!c  adjust matrix and rhs vector for boundary conditions 
      call jacbvs

      !Export sparse matrix dataset and right hand side. For test only, dsu.
      if((b_output_matrix.or.itimestep_output_matrix == mtime .or.     &
          (itimestep_output_matrix > 0 .and. mtime == 0)).and.         &
          b_enable_output) then
          if(itype_matrix_format == 0) then
              call export_arrays1d(nngl, njavs, iavs, javs, avs, bvs,  &
              uvs, .true., .true., .false., "fsflow_vs", ittot_vs)
          else if(itype_matrix_format == 1) then
              call export_mmformat(nngl, njavs, iavs, javs, avs, bvs,  &
              uvs, .true., .true., .false., "fsflow_vs", ittot_vs)
#ifdef PETSC
              call export_mmformat_gbl(nn,nngl,njavs,iavs,javs,avs,    &
                   bvs,uvs,.true.,.true.,.false.,"fsflow_vs",          &
                   nngl,nngbl,.false.,ittot_vs)
#endif
          end if
      end if
     
!cprovi--------------------------------------------------------------------
!cprovi Estimate condition number for the current matrix. 
!cprovi This is used for testing when newton iteration failed.
!cprovi--------------------------------------------------------------------    
#ifdef CONDITION_NUMBER  
         if(b_output_condition_number) then
            call cond_num_cal(nngl, njavs, iavs, javs, avs,            &
                      condition_number, condition_number_info)   
            
            if(rank == 0 .and. b_enable_output) then
            
            if (condition_number_info(1) .ge. 0) then
              write(*,"(2(a, 1pe15.6e3, 1x))")                         &
                     " classical cond. num. ",condition_number(1),     &
                     " skeel cond. num. ",condition_number(2)
              write(ilog,"(2(a, 1pe15.6e3, 1x))")                      &
                    " classical cond. num. ", condition_number(1),     &
                    " skeel cond. num. ", condition_number(2)
            else
              write(*,*)                                               &
                  ' error in estimating condition number, info(1) ',   &
                  condition_number_info(1)
              write(ilog,*)                                            &
                  ' error in estimating condition number, info(1) ',   &
                  condition_number_info(1)
            endif
            
            if(i_solver_type_flow == 1) then
              if(condition_number(1) > 1.0d10 .and.                    &
                      condition_number(2) > 1.0d10) then
                  write(*,"(a)")                                       &
                      " Warning: matrix is ill-conditioned."
                  write(ilog,"(a)")                                    &
                      " Warning: matrix is ill-conditioned."
              end if
            end if
            
            end if
         end if
#endif         

        prt_flow_jac = cputime() - prt_flow_jac
      
        prt_flow_solver = 0.0d0
        
        !! use ws209 solver
        if (i_solver_type_flow == 0) then
#ifdef PARDISO
        if(b_solver_test_pardiso) then
            !!reset value of a
    !$omp parallel                                                    &
    !$omp if (njavs > numofloops_thred_global)                        &
    !$omp num_threads(numofthreads_global)                            &
    !$omp default(shared)                                             &
    !$omp private (i)  
    !$omp do schedule(static)
            do i = 1, njavs
                avs_std(i) = avs(imapvs_std(i))
            end do
    !$omp end do
    !$omp end parallel
           
            b_redo_symbfac = .true.
100         prt_flow_symbfac_comp = cputime()
            if(bsymbolicfactor_vs .or.                                 &
                    i_symfactor_type_flow == 1) then
                call pardiso_symbolicfactorization(iparm_vs, ptvs,     &
                         nngl, njavs, iavs, javs_std, avs_std)
                n_unknown_vs = nngl
                bsymbolicfactor_vs = .false.
            end if    
            prt_flow_symbfac_comp = cputime() - prt_flow_symbfac_comp

            prt_flow_fac_comp = cputime()
            call pardiso_numfactorization(iparm_vs, ptvs, nngl,        &
                     njavs, iavs, javs_std, avs_std)
            prt_flow_fac_comp = cputime() - prt_flow_fac_comp

            prt_flow_sub_comp = cputime()
            call pardiso_substitution(ilog, msolvit_vs, itsolv,        &
                     idetail_vs, res_vs, restol_vs, deltol_vs,         &
                     over_flow, rnorm, rmupdate, iparm_vs, ptvs,       &
                     nngl, njavs, iavs, javs_std, avs_std, bvs,        &
                     uvs_std) 
            prt_flow_sub_comp = cputime() - prt_flow_sub_comp
            
            if (b_redo_symbfac .and. (itsolv > n_max_iteration_flow    &
                .or. rnorm > r_max_residual_flow .or. over_flow)) then
                bsymbolicfactor_vs = .true.
                b_redo_symbfac = .false.
                goto 100
            end if
            
        end if
#endif

#ifdef PETSC
        if(b_solver_test_petsc) then
            !only solver the local part, update the ghost value
            call solver_dd_snes_solve_flow_heat(ilog,.true.,           &
                    idetail_vs,avs,bvs,uvs_std,iavs,javs,nngl,itsolv,  &
                    over_flow,rnorm,row_idx_l2pg_vs,                   &
                    col_idx_l2pg_vs,.false.)
            over_flow_vs = over_flow
#ifdef DEBUG
            if(rank == 0) then
                write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')       &
                "fsflow-A: rank, iteration, over_flow, rnorm ",        &
                rank, itsolv, over_flow, rnorm   
            end if
#endif
        end if
#endif

#ifdef LIS
        if(b_solver_test_lis) then
          !only solver the local part, update the ghost value
          call solver_lis_solve_flow(ilog,idetail_vs,avs,bvs,          &
                      uvs_std,iavs,javs,nngl,nn,1,itsolv,over_flow,    &
                      rnorm,row_idx_l2pg_vs,col_idx_l2pg_vs,.false.)
          over_flow_vs = over_flow            
#ifdef DEBUG
          if(rank == 0 .and. b_enable_output) then
            write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')           &
                  "fsflow-A2: rank, iteration, over_flow, rnorm ",     &
                  rank, itsolv, over_flow, rnorm  
          end if
#endif
        end if
#endif 


!c  Scale [avs] and {bvs} to produce unit diagonal
!c  Generate re-ordered preconditioner [af]

      ilist = 1
      prt_flow_fac = cputime()
      call incompletefactorization (nngl,njavs,njafvs,bvs,avs,afvs,   &
                                   rwork_max,iavs,javs,iafvs,iafdvs,  &   
                                   jafvs,iwork_max(ilist),lordervs,   &
                                   invordvs,numofthreads_ws209)
      prt_flow_fac = cputime() - prt_flow_fac
      
!c  solve [avs] * {uvs} = {bvs}
      prt_flow_sub = cputime()

     
      call ws209(ilog,nngl,msolvit_vs,itsolv,idetail_vs,iavs,javs,     &
                iafvs,iafdvs,jafvs,lordervs,avs,afvs,uvs,bvs,res_vs,   &
                rwork_max,restol_vs,deltol_vs,njavs,njafvs,over_flow,  &
                rnorm,rmupdate,numofthreads_ws209,rank,b_enable_output)
      

      prt_flow_sub = cputime() - prt_flow_sub
#ifdef PARDISO
      if (b_solver_test_pardiso) then        
          call solver_results_check_output(ittot_vs, nngl, uvs,        &
                   uvs_std, "fsflow_vs_pardiso") 
      end if
#endif 

#ifdef PETSC 
      if (b_solver_test_petsc) then        
          call solver_results_check_output(ittot_vs, nngl, uvs,        &
                   uvs_std, "fsflow_vs_petsc") 
      end if
#endif 

#ifdef LIS 
      if (b_solver_test_lis) then        
          call solver_results_check_output(ittot_vs, nngl, uvs,        &
                   uvs_std, "fsflow_vs_lis") 
      end if
#endif 
        prt_flow_solver = prt_flow_symbfac+prt_flow_fac+prt_flow_sub
        !! use pardiso solver
        else if (i_solver_type_flow == 1) then
#ifdef PARDISO
    !$omp parallel                                                    &
    !$omp if (njavs > numofloops_thred_global)                        &
    !$omp num_threads(numofthreads_global)                            &
    !$omp default(shared)                                             &
    !$omp private (i)  
    !$omp do schedule(static)
            do i = 1, njavs
                avs_std(i) = avs(imapvs_std(i))
            end do
    !$omp end do
    !$omp end parallel
            
            b_redo_symbfac = .true.           
200         prt_flow_symbfac = cputime()
            if(bsymbolicfactor_vs  .or.                                &
                    i_symfactor_type_flow == 1) then
                !write(idbg, *) "pardiso symbolic factorization for fsflow line 196"
                call pardiso_symbolicfactorization(iparm_vs, ptvs,     &
                         nngl, njavs, iavs, javs_std, avs_std)
                n_unknown_vs = nngl
                bsymbolicfactor_vs = .false.
            end if    
            prt_flow_symbfac = cputime() - prt_flow_symbfac
            
            !write(idbg, *) "pardiso numerical factorization for fsflow line 200"
            prt_flow_fac = cputime()
            call pardiso_numfactorization(iparm_vs, ptvs, nngl,        &
                     njavs, iavs, javs_std, avs_std)
            prt_flow_fac = cputime() - prt_flow_fac
            
            !write(idbg, *) "pardiso substitution for fsflow line 202"
            prt_flow_sub = cputime()
            call pardiso_substitution(ilog, msolvit_vs, itsolv,        &
                     idetail_vs, res_vs,restol_vs, deltol_vs,          &
                     over_flow, rnorm, rmupdate,iparm_vs, ptvs, nngl,  &
                     njavs, iavs, javs_std, avs_std, bvs, uvs) 
            prt_flow_sub = cputime() - prt_flow_sub
            
            if (b_redo_symbfac .and. (itsolv > n_max_iteration_flow    &
                .or. rnorm > r_max_residual_flow .or. over_flow)) then
                bsymbolicfactor_vs = .true.
                b_redo_symbfac = .false.
                goto 200
                end if
                
#endif
            prt_flow_solver=prt_flow_symbfac+prt_flow_fac+prt_flow_sub
        !! use PETSc solver
        else if (i_solver_type_flow == 2) then           
#ifdef PETSC      
            prt_flow_solver = cputime()            
            !only solver the local part, update the ghost value
            call solver_dd_snes_solve_flow_heat(ilog,.true.,           &
                      idetail_vs,avs,bvs,uvs,iavs,javs,nngl,itsolv,    &
                      over_flow,rnorm,row_idx_l2pg_vs,                 &
                      col_idx_l2pg_vs,.false.)
            over_flow_vs = over_flow
            
            prt_flow_solver = cputime() - prt_flow_solver
#ifdef DEBUG
            if(rank == 0 .and. b_enable_output) then
                write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')       &
                "fsflow-B: rank, iteration, over_flow, rnorm ",        &
                rank, itsolv, over_flow, rnorm  
            end if
#endif
#endif        
        !! use LIS solver
        else if (i_solver_type_flow == 3) then           
#ifdef LIS      
          prt_flow_solver = cputime()            
          !only solver the local part, update the ghost value
          call solver_lis_solve_flow(ilog,idetail_vs,avs,bvs,          &
                      uvs,iavs,javs,nngl,nn,1,itsolv,over_flow,rnorm,  &
                      row_idx_l2pg_vs,col_idx_l2pg_vs,.false.)
          over_flow_vs = over_flow            
          prt_flow_solver = cputime() - prt_flow_solver
#ifdef DEBUG
          if(rank == 0 .and. b_enable_output) then
            write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')           &
                  "fsflow-B2: rank, iteration, over_flow, rnorm ",     &
                  rank, itsolv, over_flow, rnorm  
          end if
#endif
#endif        

        end if

        !Export sparse matrix dataset and right hand side. For test only, dsu.
        if((b_output_matrix.or.itimestep_output_matrix == mtime .or.   &
            (itimestep_output_matrix > 0 .and. mtime == 0)).and.       &
            b_enable_output) then
            if(itype_matrix_format == 0) then
                call export_arrays1d(nngl,njavs,iavs,javs,avs,bvs,     &
                       uvs,.false.,.false.,.true.,"fsflow_vs",ittot_vs)
            else if(itype_matrix_format == 1) then
                call export_mmformat(nngl,njavs,iavs,javs,avs,bvs,     &
                       uvs,.false.,.false.,.true.,"fsflow_vs",ittot_vs)
#ifdef PETSC
                call export_mmformat_gbl(nn,nngl,njavs,iavs,javs,avs,  &
                     bvs,uvs,.false.,.false.,.true.,"fsflow_vs",       &
                     nngl,nngbl,.false.,ittot_vs)
#endif
            end if
        end if 

!cdsu  debug part, use external solution (written in sequential order) to test
        !if (mtime >= 1 .and. mtime <= 4) then
        !   call export_mmformat_gbl(nn, nngl, njavs, iavs, javs,      &
        !               avs, bvs, uvs, .true., .true., .true.,         &
        !               "fsflow_vs_check",nngl,nngbl, .false., ittot_vs)
        !end if

        !if (rank == 0) then
        !  write(*,*) "-->read hydraulic head and temperature change from external"
        !end if

        !ifile = lun_get()
        !write(strfile, *) ittot_vs
        !strfile = "x_fsflow_vs_"//trim(adjustl(strfile))//"_natgbl.txt"
        !open(ifile,file=trim(strfile),status='old',form='formatted')
        !uvs = 0.0d0
        !nskip = 0
        !read(ifile,*) strdummy
        !do ivol = 1, nngl
#ifdef PETSC
        !  do iskip = 1, node_idx_lg2g(ivol) - nskip -1
        !    read(ifile,*) idummy
        !  end do
        !  nskip = node_idx_lg2g(ivol)
#endif
        !  read(ifile,*) idummy,uvs(ivol)
        !end do

        !call lun_free(ifile)
!cdsu  debug part, use external solution (written in sequential order) to test, end
 

        if (.not.transient_flow.and.itsolv.eq.msolvit_vs) then
            if(rank == 0) then      !if MPI rank 0
              write(*,'(a/a)')                                  &
                'maximum number of inner iterations exceeded',  &
                'steady state flow solution non-convergent'

              write(ilog,'(a/a)')                               &
                'maximum number of inner iterations exceeded',  &
                'steady state flow solution non-convergent'  
            end if                  !end if MPI rank 0
            
#ifdef PETSC
            call petsc_mpi_finalize
#endif
            stop
        end if
        

!c  deallocate memory for solver
      !if (i_solver_type_flow == 0) then
      !    deallocate (rwork, stat = ierr)
      !    call checkerr(ierr,'rwork',ilog)
      !
      !    deallocate (iwork, stat = ierr)
      !    call checkerr(ierr,'iwork',ilog)
      !
      !    deallocate (afvs, stat = ierr)
      !    call checkerr(ierr,'afvs',ilog)
      !end if
      if (b_dynamic_memory) then
          call memory_monitor(-sizeof(avs),'avs',.true.)
          deallocate (avs, stat = ierr)
          call checkerr(ierr,'avs',ilog)
      
          if (i_solver_type_flow == 0) then
              call memory_monitor(-sizeof(afvs),'afvs',.true.)
              deallocate (afvs, stat = ierr)
              call checkerr(ierr,'afvs',ilog)  
          end if
      end if

!c  total number of solver iterations

      itsolvtot_vs = itsolvtot_vs + itsolv

!c  terminate execution, in case of occurrence of overflow

      if (.not.over_flow) then
        !Parallelized, OpenMP, DSU
        call updatevs

!cdsu ---------------------------------------------------------------------------------------------------
!cdsu check if newton iteration is diverged based on the saved maximum update value in newton iteration
!cdsu ---------------------------------------------------------------------------------------------------
        if (b_check_div_vs .and. iter_vs >= 5) then
          iter_div = mod(iter_vs,5)
          if (iter_div == 0) then
            if (div_vs(5) > div_vs(4)*1.1d0 .and.                      &
                div_vs(4) > div_vs(3)*1.1d0 .and.                      &
                div_vs(3) > div_vs(2)*1.1d0 .and.                      &
                div_vs(2) > div_vs(1)*1.1d0) then
              reduce_timestep = .true.
            end if
          else if (iter_div == 1) then
            if (div_vs(1) > div_vs(5)*1.1d0 .and.                      &
                div_vs(5) > div_vs(4)*1.1d0 .and.                      &
                div_vs(4) > div_vs(3)*1.1d0 .and.                      &
                div_vs(3) > div_vs(2)*1.1d0) then
              reduce_timestep = .true.
            end if
          else if (iter_div == 2) then
            if (div_vs(2) > div_vs(1)*1.1d0 .and.                      &
                div_vs(1) > div_vs(5)*1.1d0 .and.                      &
                div_vs(5) > div_vs(4)*1.1d0 .and.                      &
                div_vs(4) > div_vs(3)*1.1d0) then
              reduce_timestep = .true.
            end if
          else if (iter_div == 3) then
            if (div_vs(3) > div_vs(2)*1.1d0 .and.                      &
                div_vs(2) > div_vs(1)*1.1d0 .and.                      &
                div_vs(1) > div_vs(5)*1.1d0 .and.                      &
                div_vs(5) > div_vs(4)*1.1d0) then
              reduce_timestep = .true.
            end if
          else if (iter_div == 4) then
            if (div_vs(4) > div_vs(3)*1.1d0 .and.                      &
                div_vs(3) > div_vs(2)*1.1d0 .and.                      &
                div_vs(2) > div_vs(1)*1.1d0 .and.                      &
                div_vs(1) > div_vs(5)*1.1d0) then
              reduce_timestep = .true.
            end if
          end if
          if (reduce_timestep) then
            if (steady_flow) then
              if (rank == 0) then
                write(ilog,*)                                          &
                '-------------------------------------------'
                write(ilog,*)                                          &
                '   terminated in routine fsflow            '
                write(ilog,*)                                          &
                '   newton solver diverges                  '
                write(ilog,*)                                          &
                '   bye now ...                             '
                write(ilog,*)                                          &
                '-------------------------------------------'
              end if
#ifdef PETSC
              call petsc_mpi_finalize
#endif
              stop
            else
              if (rank == 0 .and. b_enable_output .and. idetail_vs.gt.0) then
                write(*,*) 'reduce time step: newton iteration diverged'
                write(ilog,*) 'reduce time step: newton iteration diverged'
              end if
            end if
          end if
        end if

!c  max. number of iterations is exceeded and convergence tolerance
!c  not satisfied
!c  steady state problem -> terminate execution

        if ((iter_vs.ge.maxit_vs).and.(not_converged)) then
          if (steady_flow) then

            if (rank == 0 ) then
              write(ilog,*)                                       &
               '-------------------------------------------'
              write(ilog,*)                                       &
               '   terminated in routine vsflow            '
              write(ilog,*)                                       &
               '   maximum number of iterations exceeded   '
              write(ilog,*)                                       &
               '   bye now ...                             '
              write(ilog,*)                                       &
               '-------------------------------------------'
            end if

            if (rank == 0 .and. b_enable_output .and. b_enable_output_gen) then
              write(igen,*)                                       &
               '-------------------------------------------'
              write(igen,*)                                       &
               '   terminated in routine vsflow            '
              write(igen,*)                                       &
               '   maximum number of iterations exceeded   '
              write(igen,*)                                       &
               '   bye now ...                             '
              write(igen,*)                                       &
               '-------------------------------------------'
            end if

#ifdef PETSC
            call petsc_mpi_finalize
#endif
            stop
!c  transient problem -> reduce time step

          else if (transient_flow) then

            if(rank == 0  .and. b_enable_output)  then
              write(ilog,*)                                       &
               '-------------------------------------------'
              write(ilog,*)                                       &
               '   maximum number of iterations exceeded   '
              write(ilog,*)                                       &
               '             reducing time step            '
              write(ilog,*)                                       &
               '-------------------------------------------'
            end if

            reduce_timestep = .true.
          end if
        end if

      else
!c  overflow occurred
!c  steady state problem -> terminate execution
        if (steady_flow) then  
          if(rank == 0) then
            write(*,*) 'failure in solver - overflow occurred in fsflow'
            write(ilog,*) 'failure in solver - overflow occurred in fsflow'
            write(ilog,*) 'bye now ...'

            if (b_enable_output_gen) then
              write(igen,*) 'failure in solver - overflow occurred in fsflow'
              write(igen,*) 'bye now ...'
            end if
          end if

#ifdef PETSC
          call petsc_mpi_finalize
#endif
          stop
!c  transient problem -> reduce time step
        else if (transient_flow) then
          if(rank == 0 .and. b_enable_output)  then   
            write(ilog,*)'-------------------------------------------'
            write(ilog,*)'   failure in solver - overflow occurred in fsflow   '
            write(ilog,*)'             reducing time step            '
            write(ilog,*)'-------------------------------------------'
          end if              
          reduce_timestep = .true.
        end if
      end if

      prt_flow_tot = cputime() - prt_flow_tot
!c  write runtime to file
      if(rank == 0 .and. b_prtfile) then      !if MPI rank 0
        write(iprt_flow, "(i8,1x,3(i3, 1x),i8,1x,7(1pe15.6e3,2x))")    &
              mtime, iter_sia, iter_seep, iter_vs, ittot_vs,           &
              prt_flow_jac, prt_flow_symbfac, prt_flow_fac,            &
              prt_flow_sub, prt_flow_solver,                           &
              (prt_flow_tot - prt_flow_jac - prt_flow_solver),         &
              prt_flow_tot

        if(b_solver_test_pardiso) then
          write(iprt_flow_comp,"(i8,1x,3(i3,1x),i8,1x,5(1pe15.6e3,2x))") &
                mtime, iter_sia, iter_seep, iter_vs, ittot_vs,         &
                prt_flow_fac, prt_flow_sub, prt_flow_symbfac_comp,     &
                prt_flow_fac_comp, prt_flow_sub_comp
        end if
      end if                  !end if MPI rank 0

!c  reset primary and secondary unknowns for reduced time step
      if (reduce_timestep) then
        exit
      end if

      end do          !newton iteration loop

!c check if boundary nodes with influx is ponding
      if (b_water_freezing .and. b_freezing_no_pond .and.            &
          .not.reduce_timestep) then
        b_freezing_pond = .false.

        uvsmax = r0
        maxvol = i0

        do ibvs = 1, nbvs          
          ivol = iabvs(ibvs)
          if (ivol < 0) then
            cycle  
          end if
          if ((btypevs(ibvs).eq.'second' .or. btypevs(ibvs).eq.'point' .or. &
               btypevs(ibvs).eq.'seepage-second') .and. &
               uvsnew(ivol) > tol_freezing_pond(ibvs)) then
            bcondvs(ibvs) = r0
            b_freezing_pond = .true.
            reduce_timestep = .true.
            if (uvsnew(ivol) > uvsmax) then
              uvsmax = uvsnew(ivol)
              maxvol = ivol
            end if
          end if
        end do
#ifdef PETSC
        call MPI_Allreduce(b_freezing_pond,reduce_timestep,1,    &
                 MPI_LOGICAL,MPI_LOR,Petsc_Comm_World,ierrcode)
        CHKERRQ(ierrcode)

        if (reduce_timestep) then
          mpireduce_in(1) = uvsmax      !returns the reduced value
          mpireduce_in(2) = rank        !returns the rank of process that owns it
          call MPI_Allreduce(mpireduce_in, mpireduce_out, 1,               &
                             MPI_2DOUBLE_PRECISION,MPI_MAXLOC,             &
                             Petsc_Comm_World,ierrcode)
          CHKERRQ(ierrcode)
          uvsmax = mpireduce_out(1)
          mpireduce_irank = int(mpireduce_out(2))
          
          call MPI_BCAST(maxvol, 1, MPI_INTEGER4, mpireduce_irank,         &
                         Petsc_Comm_World, ierrcode) 
          CHKERRQ(ierrcode)                
        end if
#endif

        if (reduce_timestep) then
          if (rank == 0 .and. b_enable_output .and. idetail_vs.gt.0) then
#ifdef PETSC
            write(*,'(1x,a,1x,1pe11.4,1x,a,1x,i9,1x,a,1x,i6)')             &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'m, vol',maxvol,'rank',mpireduce_irank
            write(ilog,'(1x,a,1x,1pe11.4,1x,a,1x,i9,1x,a,1x,i6)')          &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'m, vol',maxvol,'rank',mpireduce_irank
#else
            write(*,'(1x,a,1x,1pe11.4,1x,a,1x,i9)')                        &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'m, vol',maxvol
            write(ilog,'(1x,a,1x,1pe11.4,1x,a,1x,i9)')                     &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'m, vol',maxvol
#endif
          end if
        end if
      end if      

!c  reset primary and secondary unknowns for reduced time step
      if (reduce_timestep) then
#ifdef OPENMP
    !$omp parallel                                                     &
    !$omp if (nngl > numofloops_thred_global)                          &
    !$omp num_threads(numofthreads_global)                             &
    !$omp default(shared)                                              &
    !$omp private (ivol)
    !$omp do schedule(static)
#endif
        do ivol=1,nngl
          uvsnew(ivol) = uvsold(ivol)
          hhead(ivol) = uvsold(ivol)+zg(ivol)
        end do
#ifdef OPENMP
    !$omp end do
    !$omp end parallel
#endif
      end if

      if (transient_flow .and. .not.reduce_timestep) then
        call tstepfs
      end if
 
      return
      end
